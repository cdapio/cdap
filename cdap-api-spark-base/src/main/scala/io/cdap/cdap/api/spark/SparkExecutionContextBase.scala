/*
 * Copyright Â© 2017-2019 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package io.cdap.cdap.api.spark

import io.cdap.cdap.api.RuntimeContext
import io.cdap.cdap.api.ServiceDiscoverer
import io.cdap.cdap.api.TaskLocalizationContext
import io.cdap.cdap.api.Transactional
import io.cdap.cdap.api.TxRunnable
import io.cdap.cdap.api.annotation.Beta
import io.cdap.cdap.api.data.batch.Split
import io.cdap.cdap.api.lineage.field.LineageRecorder
import io.cdap.cdap.api.messaging.MessagingContext
import io.cdap.cdap.api.metadata.MetadataReader
import io.cdap.cdap.api.metadata.MetadataWriter
import io.cdap.cdap.api.metrics.Metrics
import io.cdap.cdap.api.plugin.PluginContext
import io.cdap.cdap.api.schedule.TriggeringScheduleInfo
import io.cdap.cdap.api.security.store.SecureStore
import io.cdap.cdap.api.spark.dynamic.SparkInterpreter
import io.cdap.cdap.api.workflow.WorkflowInfo
import io.cdap.cdap.api.workflow.WorkflowToken
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.tephra.TransactionFailureException

import java.io.IOException
import scala.reflect.ClassTag
/**
  * Spark program execution context. User Spark program can interact with CDAP through this context.
  */
@Beta
trait SparkExecutionContextBase extends RuntimeContext
  with Transactional with MetadataReader with MetadataWriter with LineageRecorder {

  /**
    * @return The specification used to configure this Spark job instance.
    */
  def getSpecification: SparkSpecification

  /**
    * Returns the logical start time of this Spark job. Logical start time is the time when this Spark
    * job is supposed to start if this job is started by the scheduler. Otherwise it would be the current time when the
    * job runs.
    *
    * @return Time in milliseconds since epoch time (00:00:00 January 1, 1970 UTC).
    */
  def getLogicalStartTime: Long

  /**
   * Returns the termination time of this Spark job as timestamp in milliseconds.
   * The termination time is the time where this Spark job will be forced to termination
   * The value is only available when stop was requested on this Spark job.
   *
   * @return Time in milliseconds since epoch time (00:00:00 January 1, 1970 UTC).
   * @throws IllegalStateException if stop was not requested
   */
  def getTerminationTime: Long

  /**
    * Returns a [[scala.Serializable]] [[io.cdap.cdap.api.ServiceDiscoverer]] for Service Discovery
    * in Spark Program which can be passed in Spark program's closures.
    *
    * @return A [[scala.Serializable]] [[io.cdap.cdap.api.ServiceDiscoverer]]
    */
  def getServiceDiscoverer: ServiceDiscoverer

  /**
    * Returns a [[scala.Serializable]] [[io.cdap.cdap.api.metrics.Metrics]] which can be used to emit
    * custom metrics from the Spark program. This can also be passed in closures and workers can emit their own metrics.
    *
    * @return A [[scala.Serializable]] [[io.cdap.cdap.api.metrics.Metrics]]
    */
  def getMetrics: Metrics

  /**
    * Returns a [[scala.Serializable]] [[io.cdap.cdap.api.plugin.PluginContext]] which can be used to request
    * for plugins instances. The instance returned can also be used in Spark program's closures.
    *
    * @return A [[scala.Serializable]] [[io.cdap.cdap.api.plugin.PluginContext]]
    */
  def getPluginContext: PluginContext

  /**
    * Returns a [[scala.Serializable]] [[io.cdap.cdap.api.security.store.SecureStore]] which can be used to request
    * for plugins instances. The instance returned can also be used in Spark program's closures.
    *
    * @return A [[scala.Serializable]] [[io.cdap.cdap.api.plugin.PluginContext]]
    */
  def getSecureStore: SecureStore

  /**
    * Returns a [[io.cdap.cdap.api.messaging.MessagingContext]] which can be used to interact with the transactional
    * messaging system. Currently the returned instance can only be used in the Spark driver process.
    *
    * @return A [[io.cdap.cdap.api.messaging.MessagingContext]]
    */
  def getMessagingContext: MessagingContext

  /**
    * Returns the [[io.cdap.cdap.api.workflow.WorkflowToken]] if the Spark program
    * is started from a [[io.cdap.cdap.api.workflow.Workflow]].
    *
    * @return An optional [[io.cdap.cdap.api.workflow.WorkflowToken]] associated with
    *         the current [[io.cdap.cdap.api.workflow.Workflow]]
    */
  def getWorkflowToken: Option[WorkflowToken]

  /**
    * Returns the [[io.cdap.cdap.api.workflow.WorkflowInfo]] if the Spark program
    * is started from a [[io.cdap.cdap.api.workflow.Workflow]].
    *
    * @return An optional [[io.cdap.cdap.api.workflow.WorkflowInfo]] associated with
    *         the current [[io.cdap.cdap.api.workflow.Workflow]]
    */
  def getWorkflowInfo: Option[WorkflowInfo]

  /**
    * Returns the [[io.cdap.cdap.api.TaskLocalizationContext]] that gives access to files that were localized
    * by [[io.cdap.cdap.api.spark.Spark]] `beforeSubmit` method.
    */
  def getLocalizationContext: TaskLocalizationContext

  /**
    * Creates a [[org.apache.spark.rdd.RDD]] from the given [[io.cdap.cdap.api.dataset.Dataset]].
    * Using the implicit object io.cdap.cdap.api.spark.SparkMain.SparkProgramContextFunctions is preferred.
    *
    * @param sc the [[org.apache.spark.SparkContext]] to use
    * @param datasetName name of the Dataset
    * @param arguments arguments for the Dataset
    * @param splits an [[scala.Option]] of [[scala.collection.Iterable]] of [[io.cdap.cdap.api.data.batch.Split]]
    *               to be used to compute the partitions of the RDD
    * @tparam K key type
    * @tparam V value type
    * @return A new [[org.apache.spark.rdd.RDD]] instance that reads from the given Dataset.
    * @throws io.cdap.cdap.api.data.DatasetInstantiationException if the Dataset doesn't exist
    */
  def fromDataset[K: ClassTag, V: ClassTag](sc: SparkContext,
                                            datasetName: String,
                                            arguments: Map[String, String],
                                            splits: Option[Iterable[_ <: Split]]): RDD[(K, V)]

  /**
    * Creates a [[org.apache.spark.rdd.RDD]] from the given [[io.cdap.cdap.api.dataset.Dataset]].
    * Using the implicit object io.cdap.cdap.api.spark.SparkMain.SparkProgramContextFunctions is preferred.
    *
    * @param sc the [[org.apache.spark.SparkContext]] to use
    * @param namespace namespace in which the dataset exists
    * @param datasetName name of the Dataset
    * @param arguments arguments for the Dataset
    * @param splits an [[scala.Option]] of [[scala.collection.Iterable]] of [[io.cdap.cdap.api.data.batch.Split]]
    *               to be used to compute the partitions of the RDD
    * @tparam K key type
    * @tparam V value type
    * @return A new [[org.apache.spark.rdd.RDD]] instance that reads from the given Dataset.
    * @throws io.cdap.cdap.api.data.DatasetInstantiationException if the Dataset doesn't exist
    */
  def fromDataset[K: ClassTag, V: ClassTag](sc: SparkContext,
                                            namespace: String,
                                            datasetName: String,
                                            arguments: Map[String, String],
                                            splits: Option[Iterable[_ <: Split]]): RDD[(K, V)]

  /**
    * Saves the given [[org.apache.spark.rdd.RDD]] to the given [[io.cdap.cdap.api.dataset.Dataset]].
    * Using the implicit object io.cdap.cdap.api.spark.SparkMain.SparkProgramRDDFunctions is preferred.
    *
    * @param rdd the [[org.apache.spark.rdd.RDD]] to operate on
    * @param datasetName name of the Dataset
    * @param arguments arguments for the Dataset
    * @throws io.cdap.cdap.api.data.DatasetInstantiationException if the Dataset doesn't exist
    */
  def saveAsDataset[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)],
                                              datasetName: String, arguments: Map[String, String]): Unit

  /**
    * Saves the given [[org.apache.spark.rdd.RDD]] to the given [[io.cdap.cdap.api.dataset.Dataset]].
    * Using the implicit object io.cdap.cdap.api.spark.SparkMain.SparkProgramRDDFunctions is preferred.
    *
    * @param rdd the [[org.apache.spark.rdd.RDD]] to operate on
    * @param namespace namespace for the Dataset
    * @param datasetName name of the Dataset
    * @param arguments arguments for the Dataset
    * @throws io.cdap.cdap.api.data.DatasetInstantiationException if the Dataset doesn't exist
    */
  def saveAsDataset[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)], namespace: String, datasetName: String,
                                              arguments: Map[String, String]): Unit

  /**
    * Transactions with a custom timeout are not supported in Spark.
    *
    * @throws TransactionFailureException always
    */
  def execute(timeoutInSeconds: Int, runnable: TxRunnable): Unit

  /**
    * Creates a new instance of [[io.cdap.cdap.api.spark.dynamic.SparkInterpreter]] for Scala code compilation and
    * interpretation.
    *
    * @return a new instance of [[io.cdap.cdap.api.spark.dynamic.SparkInterpreter]]
    * @throws java.io.IOException if failed to create a local directory for storing the compiled class files
    */
  @throws(classOf[IOException])
  def createInterpreter(): SparkInterpreter

  /**
    * Get the information of the schedule that launches this Spark program, if there is any.
    *
    * @return a instance of [[scala.Some]] containing [[io.cdap.cdap.api.schedule.TriggeringScheduleInfo]]
    *         of the schedule that launches this Spark program. Return [[scala.None]] if the program is
    *         not launched by a schedule
    */
  def getTriggeringScheduleInfo: Option[TriggeringScheduleInfo]

  /**
    * Returns a new instance of [[io.cdap.cdap.api.spark.JavaSparkExecutionContext]].
    */
  def toJavaSparkExecutionContext(): JavaSparkExecutionContext
}
