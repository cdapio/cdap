/*
 * Copyright Â© 2016 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package io.cdap.cdap.etl.spark.function;

import io.cdap.cdap.api.data.format.StructuredRecord;
import io.cdap.cdap.api.data.schema.Schema;
import io.cdap.cdap.etl.api.Emitter;
import io.cdap.cdap.etl.api.Transformation;
import io.cdap.cdap.etl.api.batch.BatchAutoJoiner;
import io.cdap.cdap.etl.api.batch.BatchJoiner;
import io.cdap.cdap.etl.api.batch.BatchJoinerRuntimeContext;
import io.cdap.cdap.etl.api.join.AutoJoinerContext;
import io.cdap.cdap.etl.api.join.JoinCondition;
import io.cdap.cdap.etl.api.join.JoinDefinition;
import io.cdap.cdap.etl.api.join.JoinStage;
import io.cdap.cdap.etl.common.Constants;
import io.cdap.cdap.etl.common.DefaultEmitter;
import io.cdap.cdap.etl.common.TrackedTransform;
import io.cdap.cdap.etl.common.plugin.JoinerBridge;
import org.apache.spark.api.java.function.PairFlatMapFunction;
import scala.Tuple2;

import java.util.Iterator;

/**
 * Function that uses a BatchJoiner to perform the joinOn part of the join.
 * Non-serializable fields are lazily created since this is used in a Spark closure.
 *
 * @param <JOIN_KEY> the type of join key
 * @param <INPUT_RECORD> the type of input records to the join stage
 */
public class JoinOnFunction<JOIN_KEY, INPUT_RECORD>
  implements PairFlatMapFunction<INPUT_RECORD, JOIN_KEY, INPUT_RECORD> {

  private final PluginFunctionContext pluginFunctionContext;
  private final FunctionCache functionCache;
  private final String inputStageName;
  private transient TrackedTransform<INPUT_RECORD, Tuple2<JOIN_KEY, INPUT_RECORD>> joinFunction;
  private transient DefaultEmitter<Tuple2<JOIN_KEY, INPUT_RECORD>> emitter;

  public JoinOnFunction(PluginFunctionContext pluginFunctionContext,
                        FunctionCache functionCache,
                        String inputStageName) {
    this.pluginFunctionContext = pluginFunctionContext;
    this.functionCache = functionCache;
    this.inputStageName = inputStageName;
  }

  @Override
  public Iterator<Tuple2<JOIN_KEY, INPUT_RECORD>> call(INPUT_RECORD input) throws Exception {
    if (joinFunction == null) {
      joinFunction = new TrackedTransform<>(functionCache.getValue(this::createInitializedJoinOnTransform),
                                            pluginFunctionContext.createStageMetrics(),
                                            Constants.Metrics.RECORDS_IN,
                                            null, pluginFunctionContext.getDataTracer(),
                                            pluginFunctionContext.getStageStatisticsCollector());
      emitter = new DefaultEmitter<>();
    }
    emitter.reset();
    joinFunction.transform(input, emitter);
    return emitter.getEntries().iterator();
  }

  private JoinOnTransform<INPUT_RECORD, JOIN_KEY> createInitializedJoinOnTransform() throws Exception {
    Object plugin = pluginFunctionContext.createPlugin();
    BatchJoiner<JOIN_KEY, INPUT_RECORD, Object> joiner;
    boolean filterNullKeys = false;
    if (plugin instanceof BatchAutoJoiner) {
      BatchAutoJoiner autoJoiner = (BatchAutoJoiner) plugin;
      AutoJoinerContext autoJoinerContext = pluginFunctionContext.createAutoJoinerContext();
      JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);
      autoJoinerContext.getFailureCollector().getOrThrowException();
      String stageName = pluginFunctionContext.getStageName();
      if (joinDefinition == null) {
        throw new IllegalStateException(String.format(
          "Join stage '%s' did not specify a join definition. " +
            "Check with the plugin developer to ensure it is implemented correctly.", stageName));
      }
      JoinCondition condition = joinDefinition.getCondition();
      /*
         Filter out the record if it comes from an optional stage
         and the key is null, or if any of the fields in the key is null.
         For example, suppose we are performing a left outer join on:

          A (id, name) = (0, alice), (null, bob)
          B (id, email) = (0, alice@example.com), (null, placeholder@example.com)

         The final output should be:

         joined (A.id, A.name, B.email) = (0, alice, alice@example.com), (null, bob, null, null)

         that is, the bob record should not be joined to the placeholder@example email, even though both their
         ids are null.
       */
      if (condition.getOp() == JoinCondition.Op.KEY_EQUALITY && !((JoinCondition.OnKeys) condition).isNullSafe()) {
        filterNullKeys = joinDefinition.getStages().stream()
          .filter(s -> !s.isRequired())
          .map(JoinStage::getStageName)
          .anyMatch(s -> s.equals(inputStageName));
      }
      joiner = new JoinerBridge(stageName, autoJoiner, joinDefinition);
    } else {
      joiner = (BatchJoiner<JOIN_KEY, INPUT_RECORD, Object>) plugin;
      BatchJoinerRuntimeContext context = pluginFunctionContext.createBatchRuntimeContext();
      joiner.initialize(context);
    }

    return new JoinOnTransform<>(joiner, inputStageName, filterNullKeys);
  }

  private static class JoinOnTransform<INPUT, JOIN_KEY> implements Transformation<INPUT, Tuple2<JOIN_KEY, INPUT>> {
    private final BatchJoiner<JOIN_KEY, INPUT, ?> joiner;
    private final String inputStageName;
    private final boolean filterNullKeys;

    JoinOnTransform(BatchJoiner<JOIN_KEY, INPUT, ?> joiner, String inputStageName, boolean filterNullKeys) {
      this.joiner = joiner;
      this.inputStageName = inputStageName;
      this.filterNullKeys = filterNullKeys;
    }

    @Override
    public void transform(INPUT inputValue, Emitter<Tuple2<JOIN_KEY, INPUT>> emitter) throws Exception {
      JOIN_KEY key = joiner.joinOn(inputStageName, inputValue);
      if (filterNullKeys) {
        if (key == null) {
          return;
        }
        if (key instanceof StructuredRecord) {
          StructuredRecord keyRecord = (StructuredRecord) key;
          for (Schema.Field field : keyRecord.getSchema().getFields()) {
            if (keyRecord.get(field.getName()) == null) {
              return;
            }
          }
        }
      }
      emitter.emit(new Tuple2<>(key, inputValue));
    }
  }
}
