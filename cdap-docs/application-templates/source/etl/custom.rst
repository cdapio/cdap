.. meta::
    :author: Cask Data, Inc.
    :copyright: Copyright © 2015 Cask Data, Inc.

.. _advanced-custom-app-template:

===========================
Creating Custom ETL Plugins
===========================

Overview
========
This section is intended for developers writing custom ETL Plugins.
Users of these should refer to the :ref:`Application Templates
<apptemplates-index>`.


Creating Custom ETL Plugins
===========================

CDAP provides for the creation of custom ETL Plugins for batch/realtime sources/sinks and
transformations to extend the existing ``ETLBatch`` and ``ETLRealtime`` Application Templates.

To make a custom plugin available to one of the Application Templates (and thus available
to any Adapter created from one of the Templates), the plugin should be packaged as a bundle jar
and then placed in the appropriate directory. 

.. _advanced-custom-app-template-installation-directory:

Installation Directory
----------------------

- **Standalone mode:** ``$CDAP_INSTALL_DIR/templates/plugins/<template-type>``

- **Distributed mode:** The plugin jars should be placed in the local file system and the path
  can be provided to CDAP by setting the property ``app.template.dir`` in
  ``cdap-site.xml``. The default path is: ``/opt/cdap/master/templates/plugins/<template-type>``

where ``template-type`` is one of the ETL App Template types (``ETLBatch`` or ``ETLRealtime``)

The CDAP Standalone should be restarted for this change to take effect in Standalone mode,
and ``cdap-master`` Services should be restarted in the Distributed mode.


Plugin Types and Maven Archetypes
=================================

In ETL Templates, there are three plugin types:

- Source
- Sink
- Transformation

There are five different Maven archetypes available for starting a plugin project:

- Batch Source
- Batch Sink
- Realtime Source
- Realtime Sink
- Transformation

Available Annotations
---------------------
These annotations may be used with the Plugin classes:

- ``@Plugin``: The class to be exposed as a Plugin needs to be annotated with the ``@Plugin``
  annotation and the type of the plugin should be specified (*source*, *sink*, *transformation*).
  By default, the plugin type will be ‘plugin’.

- ``@Name``: Annotation used to name the Plugin as well as the properties in the
  Configuration class of the Plugin.

- ``@Description``: Annotation used to add a description.

- ``@Nullable`` - This annotation indicates that the specific configuration property is
  optional. Such a plugin class can be used without that property being specified.


Creating a Batch Source Plugin
------------------------------
A Batch Source Plugin can be created from a Maven archetype. This command will create a
project for the Plugin from the archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-batch-source-archetype \\
          -DarchetypeVersion=\ |release|

In order to implement a Batch Source (to be used in the ETL Batch Template), you extend
the BatchSource class. You need to define the types of the KEY and VALUE that the Batch
Source will receive and the type of object that the Batch Source will emit to the
subsequent stage (which could be either a Transform or a BatchSink). After defining
the types, only one method is required to be implemented:

  ``prepareRun()``

Methods
.......

- ``prepareRun()``: Used to configure the Hadoop Job configuration (for example, set the
  ``InputFormatClass``).
- ``configurePipeline()``: Used to create any Streams or Datasets that are required by this 
  Batch Source.
- ``initialize()``: Initialize the Batch Source. Guaranteed to be executed before any call
  to the plugin’s ``transform`` method.
- ``transform()``: This method will be called for every input key-value pair generated by 
  the Batch Job. By default, the value is emitted to the subsequent stage.

Example::

  @Plugin(type = "source")
  @Name("MyBatchSource")
  @Description("Demo Source")
  public class MyBatchSource extends BatchSource<LongWritable, String, String> {

    @Override
    public void prepareRun(BatchSourceContext context) {
      Job job = context.getHadoopJob();
      job.setInputFormatClass(...);
      // Other Hadoop job configuration related to Input
    }
  }


Creating a Batch Sink Plugin
----------------------------
A Batch Sink Plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-batch-sink-archetype \\
          -DarchetypeVersion=\ |release|

In order to implement a Batch Sink (to be used in the ETL Batch template), you extend the
BatchSink class. Similar to a BatchSource, you need to define the types of the KEY and
VALUE that the BatchSink will write in the Batch job and the type of object that it will
accept from the previous stage (which could be either a ``Transform`` or a ``BatchSource``).

After defining the types, only one method is required to be implemented:

  ``prepareRun()``

Methods
.......

- ``prepareRun()``: Used to configure the Hadoop Job configuration (for ex, set ``OutputFormatClass``).
- ``configurePipeline()``: Used to create any datasets that are required by this Batch Sink.
- ``initialize()``: Initialize the Batch Sink runtime. Guaranteed to be executed before
  any call to the plugin’s ``transform`` method.
- ``transform()``: This method will be called for every object that is received from the
  previous stage. The logic inside the method will transform the object to the key-value
  pair expected by the BatchSink's output format. If you don't override this method, the
  incoming object is set as the Key and the Value is set to null.

Example::

  @Plugin(type = "sink")
  @Name("MyBatchSink")
  @Description("Demo Sink")
  public class MyBatchSink extends BatchSink<String, String, NullWritable> {

    @Override
    public void prepareRun(BatchSinkContext context) {
      Job job = context.getHadoopJob();
      job.setOutputFormatClass(...);
      // Other Hadoop job configuration related to Output
    }
  }


Creating a Realtime Source Plugin
---------------------------------
A Realtime Source Plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-realtime-source-archetype \\
          -DarchetypeVersion=\ |release|

The only method that needs to be implemented is:

	``poll()``

Methods 
.......

- ``initialize()``: Initialize the Realtime Source runtime. Guaranteed to be executed
  before any call to the poll method. Usually used to setup the connection to external
  sources.
- ``poll()``: Poll method will be invoked during the run of the Adapter and in each call,
  the source is expected to emit zero or more objects for the next stage to process. 
- ``destroy()``: Cleanup method executed during the shutdown of the Source. Could be used
  to tear down any external connections made during the initialize method.

Example::

  /**
   * Realtime Source to poll data from external sources.
   */
  @Plugin(type = "source")
  @Name("Source")
  @Description("Realtime Source")
  public class Source extends RealtimeSource<StructuredRecord> {

    private final SourceConfig config;

    public Source(SourceConfig config) {
      this.config = config;
    }

    /**
     * Config class for Source.
     */
    public static class SourceConfig extends PluginConfig {

      @Name("param")
      @Description("Source Param")
      private String param;
      // Note:  only primitives (included boxed types) and string are the types that are supported

    }
  
    @Nullable
    @Override
    public SourceState poll(Emitter<StructuredRecord> writer, SourceState currentState) {
      // Poll for new data
      // Write structured record to the writer
      // writer.emit(writeDefaultRecords(writer);
      return currentState;
    }

    @Override
    public void initialize(RealtimeContext context) throws Exception {
      super.initialize(context);
      // Get Config param and use to initialize
      // String param = config.param
      // Perform init operations, external operations etc.
    }

    @Override
    public void destroy() {
      super.destroy();
      // Handle destroy lifecycle
    }

    private void writeDefaultRecords(Emitter<StructuredRecord> writer){
      Schema.Field bodyField = Schema.Field.of("body", Schema.of(Schema.Type.STRING));
      StructuredRecord.Builder recordBuilder = StructuredRecord.builder(Schema.recordOf("defaultRecord", bodyField));
      recordBuilder.set("body", "Hello");
      writer.emit(recordBuilder.build());
    }
  }


Creating a Realtime Sink Plugin
-------------------------------
A Realtime Sink Plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-realtime-sink-archetype \\
          -DarchetypeVersion=\ |release|

The only method that needs to be implemented is:

 ``write()``

Methods

- ``initialize()``: Initialize the Realtime Sink runtime. Guaranteed to be executed before
  any call to the ``write`` method. 
- ``write()``: The write method will be invoked for a set of objects that needs to be
  persisted. A ``DataWriter`` object can be used to write data to CDAP Streams and/or Datasets.
  The method is expected to return the number of objects written; this is used for collecting
  metrics.
- ``destroy()``: Cleanup method executed during the shutdown of the Sink. 

Example::

  @Plugin(type = "sink")
  @Name("Demo")
  @Description("Demo Realtime Sink")
  public class DemoSink extends RealtimeSink<String> {

    @Override
    public int write(Iterable<String> objects, DataWriter dataWriter) {
      int written = 0;
      for (String object : objects) {
        written += 1;
        . . .
      }
      return written;
    }
  }


Creating a Transformation Plugin
--------------------------------
In ETL Templates, a transformation operation is applied on one object at a time,
converting it into one or more transformed outputs. A Transformation plugin can be created
using this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-transform-archetype \\
          -DarchetypeVersion=\ |release|


The only method that needs to be implemented is:

	``transform()``

Methods
.......

- ``initialize()``: Used to perform any initialization step that might be required during
  the runtime of the ``Transform``. It is guaranteed that this method will be invoked
  before the ``transform`` method.
- ``transform()``: Transform method contains the logic that will be applied on each
  incoming data object. An emitter can be used to pass the results to the subsequent stage
  (which could be either another ``Transform`` or a ``Sink``).
- ``destroy()``: Used to perform any cleanup before the Adapter shuts down.

Below is an example of a ``DuplicateTransform`` that emits copies of the incoming record
based on the value in the record. In addition, a user metric indicating the number of
copies in each transform is emitted. The user metrics can be queried by using the CDAP 
:ref:`RESTful API<http-restful-api-apptemplates-adapter-metrics>`::


  @Plugin(type = "transform")
  @Name("Duplicator")
  @Description("Transformation Example that makes copies")

  public class DuplicateTransform extends Transform<StructuredRecord, StructuredRecord> {
  
  private final Config config;

    public static final class Config extends PluginConfig {
    
      @Name("count")
      @Description("Field that indicates number of copies to make")
      private String fieldName; 
    } 
  
    @Override
    public void transform(StructuredRecord input, Emitter<StructuredRecord> emitter) {
      Integer copies = input.get(config.fieldName);
      for (int i = 0; i < copies; i++) {
        emitter.emit(input);
      }
      getContext().getMetrics().count("copies", copies);
    }

    @Override
    public void destroy() {
    
    }
  }


Test Framework for Adapters
===========================

To unit test an Adapter, you can include ``cdap-test`` in your ``pom.xml`` and extend
``TestBase``. This will give you access to the ``addTemplatePlugins``, ``deployTemplate``,
and ``createAdapter`` methods.  Generally, you will first add Plugins, deploy a Template, and
then create an Adapter using the template. See these methods’ corresponding Javadocs for
additional information.

Creating an adapter will give you an ``AdapterManager`` which can be used to start and stop an
Adapter, as well as wait for runs to finish. Other than that, you can use normal ``TestBase``
methods to obtain Streams or Datasets and verify that they have the correct data.


Source State in Realtime Source
===============================

Realtime Adapters are executed in Workers. During failure, there is the possibility that
the data that is emitted from the Source will not be processed by subsequent stages. In
order to avoid such data loss, SourceState can be used to persist the information about
the external source (for example, offset) if supported by the Source. 

In case of failure, when the poll method is invoked, the offset last persisted is passed
to the poll method, which can be used to fetch the data from the last processed point. The
updated Source State information is returned by the poll method. After the data is
processed by any Transformations and then finally persisted by the Sink, the new Source
State information is also persisted. This ensures that there will be no data loss in case
of failures.

::

  @Plugin(type = "source")
  @Name("Demo")
  @Description("Demo Realtime Source")
  public class DemoSource extends RealtimeSource<String> {
    private static final Logger LOG = LoggerFactory.getLogger(TestSource.class);
    private static final String COUNT = "count";

    @Nullable
    @Override
    public SourceState poll(Emitter<String> writer, SourceState currentState) {
      try {
        TimeUnit.MILLISECONDS.sleep(100);
      } catch (InterruptedException e) {
        LOG.error("Some Error in Source");
      }

      int prevCount;
      if (currentState.getState(COUNT) != null) {
        prevCount = Bytes.toInt(currentState.getState(COUNT));
        prevCount++;
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      } else {
        prevCount = 1;
        currentState = new SourceState();
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      }

      LOG.info("Emitting data! {}", prevCount);
      writer.emit("Hello World!");
      return currentState;
    }
  }


Plugin Packaging
================

A Plugin is packaged as a JAR file, which contains the plugin class and its dependencies
inside. CDAP uses the "Export-Package" attribute in the JAR file manifest to determine
which classes are *visible*. A *visible* class is one that can be used by another class
that is not from the plugin JAR itself. This means the Java package which the plugin class
is in must be listed in "Export-Package", otherwise the plugin class will not be visible,
and hence no one will be able to use it.

By using one of the ``etl-plugin`` Maven archetypes, your project will be set up to generate
the required JAR manifest. If you move the plugin class to a different Java package after
the project is created, you will need to modify the configuration of the
``maven-bundle-plugin`` in the ``pom.xml`` file to reflect the package name changes.

If you are developing plugins for ``ETLBatch``, be aware that for classes inside the plugin
JAR that you have added to the Hadoop Job configuration directly (for example, your custom
``InputFormat`` class), you will need to add the Java packages of those classes to the
"Export-Package" as well. This is to ensure those classes are visible to the Hadoop
MapReduce framework during the Adapter execution. Otherwise, the execution will typically
fail with a ``ClassNotFoundException``.
