/*
 * Copyright Â© 2022 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package io.cdap.cdap.storage.spanner;

import com.google.cloud.ByteArray;
import com.google.cloud.spanner.Key;
import com.google.cloud.spanner.KeyRange;
import com.google.cloud.spanner.KeySet;
import com.google.cloud.spanner.ResultSet;
import com.google.cloud.spanner.Statement;
import com.google.cloud.spanner.Struct;
import com.google.cloud.spanner.TransactionContext;
import com.google.cloud.spanner.Value;
import io.cdap.cdap.api.dataset.lib.AbstractCloseableIterator;
import io.cdap.cdap.api.dataset.lib.CloseableIterator;
import io.cdap.cdap.spi.data.InvalidFieldException;
import io.cdap.cdap.spi.data.StructuredRow;
import io.cdap.cdap.spi.data.StructuredTable;
import io.cdap.cdap.spi.data.table.StructuredTableSchema;
import io.cdap.cdap.spi.data.table.field.Field;
import io.cdap.cdap.spi.data.table.field.FieldType;
import io.cdap.cdap.spi.data.table.field.FieldValidator;
import io.cdap.cdap.spi.data.table.field.Fields;
import io.cdap.cdap.spi.data.table.field.Range;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.Optional;
import java.util.Set;
import java.util.function.Function;
import java.util.stream.Collectors;
import java.util.stream.IntStream;
import javax.annotation.Nullable;

/**
 * A {@link StructuredTable} implementation backed by GCP Cloud Spanner.
 */
public class SpannerStructuredTable implements StructuredTable {

  private static final Logger LOG = LoggerFactory.getLogger(SpannerStructuredTable.class);

  private final TransactionContext transactionContext;
  private final StructuredTableSchema schema;
  private final FieldValidator fieldValidator;

  public SpannerStructuredTable(TransactionContext transactionContext, StructuredTableSchema schema) {
    this.transactionContext = transactionContext;
    this.schema = schema;
    this.fieldValidator = new FieldValidator(schema);
  }

  @Override
  public void upsert(Collection<Field<?>> fields) throws InvalidFieldException {
    Map<String, Field<?>> fieldMap = fields.stream().collect(Collectors.toMap(Field::getName, Function.identity()));
    List<Field<?>> primaryKeyFields = new ArrayList<>();

    for (String key : schema.getPrimaryKeys()) {
      Field<?> field = fieldMap.get(key);
      if (field == null) {
        throw new InvalidFieldException(schema.getTableId(), key, "Missing primary key field " + key);
      }
      primaryKeyFields.add(field);
    }

    // Cloud Spanner doesn't support upsert. The best we can do is to read the existing row and update it if it exists
    // in the same transaction.
    Optional<StructuredRow> row = read(primaryKeyFields, Collections.singleton(primaryKeyFields.get(0).getName()));
    if (row.isPresent()) {
      update(fields);
    } else {
      insert(fields);
    }
  }

  @Override
  public void update(Collection<Field<?>> fields) throws InvalidFieldException {
    List<Field<?>> primaryKeyFields = new ArrayList<>();
    List<Field<?>> updateFields = new ArrayList<>();
    Set<String> fieldNames = new HashSet<>();

    for (Field<?> field : fields) {
      fieldValidator.validateField(field);
      if (schema.isPrimaryKeyColumn(field.getName())) {
        primaryKeyFields.add(field);
      } else {
        updateFields.add(field);
      }
      fieldNames.add(field.getName());
    }

    if (!fieldNames.containsAll(schema.getPrimaryKeys())) {
      throw new InvalidFieldException(schema.getTableId(), fields,
                                      String.format("Given fields %s do not contain all the " +
                                                      "primary keys %s", fieldNames, schema.getPrimaryKeys()));
    }

    String sql = "UPDATE " + escapeName(schema.getTableId().getName())
      + " SET " + updateFields.stream().map(this::fieldToParam).collect(Collectors.joining(", "))
      + " WHERE " + primaryKeyFields.stream().map(this::fieldToParam).collect(Collectors.joining(" AND "));

    LOG.trace("Updating row: {}", sql);

    Statement statement = fields.stream()
      .reduce(Statement.newBuilder(sql),
              (builder, field) -> builder.bind(field.getName()).to(getValue(field)),
              (builder1, builder2) -> builder1)
      .build();

    transactionContext.executeUpdate(statement);
  }

  @Override
  public Optional<StructuredRow> read(Collection<Field<?>> keys) throws InvalidFieldException {
    return read(keys, schema.getFieldNames());
  }

  @Override
  public Optional<StructuredRow> read(Collection<Field<?>> keys,
                                      Collection<String> columns) throws InvalidFieldException {
    if (columns.isEmpty()) {
      throw new IllegalArgumentException("No column is specified to read");
    }
    fieldValidator.validatePrimaryKeys(keys, false);

    Set<String> missingColumns = columns.stream()
      .filter(f -> !schema.getFieldNames().contains(f))
      .collect(Collectors.toSet());

    if (!missingColumns.isEmpty()) {
      throw new IllegalArgumentException("Some columns do not exists in the table schema " + missingColumns);
    }

    // Adds all the keys to the result column as well. This is mirroring the PostgreSQL implementation.
    Set<String> queryColumns = keys.stream().map(Field::getName).collect(Collectors.toSet());
    queryColumns.addAll(columns);

    Struct row = transactionContext.readRow(schema.getTableId().getName(), createKey(keys), queryColumns);
    return Optional.ofNullable(row).map(r -> new SpannerStructuredRow(schema, r));
  }

  @Override
  public CloseableIterator<StructuredRow> scan(Range keyRange, int limit) throws InvalidFieldException {
    fieldValidator.validatePrimaryKeys(keyRange.getBegin(), true);
    fieldValidator.validatePrimaryKeys(keyRange.getEnd(), true);

    KeySet keySet;
    if (keyRange.getBegin().isEmpty() && keyRange.getEnd().isEmpty()) {
      keySet = KeySet.all();
    } else {
      keySet = KeySet.range(KeyRange.newBuilder()
                              .setStart(getKey(keyRange.getBegin()))
                              .setStartType(getEndpoint(keyRange.getBeginBound()))
                              .setEnd(getKey(keyRange.getEnd()))
                              .setEndType(getEndpoint(keyRange.getEndBound()))
                              .build());
    }

    return new ResultSetIterator(schema, transactionContext.read(schema.getTableId().getName(),
                                                                 keySet, schema.getFieldNames()));
  }

  @Override
  public CloseableIterator<StructuredRow> scan(Field<?> index) throws InvalidFieldException {
    fieldValidator.validateField(index);
    if (!schema.isIndexColumn(index.getName())) {
      throw new InvalidFieldException(schema.getTableId(), index.getName(), "is not an indexed column");
    }

    KeySet keySet = KeySet.singleKey(createKey(Collections.singleton(index)));
    String indexName = SpannerStructuredTableAdmin.getIndexName(schema.getTableId(), index.getName());
    ResultSet resultSet = transactionContext.readUsingIndex(schema.getTableId().getName(), indexName,
                                                            keySet, schema.getFieldNames());
    return new ResultSetIterator(schema, resultSet);
  }

  @Override
  public CloseableIterator<StructuredRow> multiScan(Collection<Range> keyRanges,
                                                    int limit) throws InvalidFieldException {
    // If nothing to scan, just return
    if (keyRanges.isEmpty()) {
      return CloseableIterator.empty();
    }

    Map<String, Value> parameters = new HashMap<>();
    String whereClause = getRangesWhereClause(keyRanges, parameters);
    if (whereClause == null) {
      return scan(Range.all(), limit);
    }

    Statement.Builder builder = Statement.newBuilder(
      "SELECT * FROM " + escapeName(schema.getTableId().getName())
       + " WHERE " + whereClause
       + " ORDER BY " + schema.getPrimaryKeys().stream().map(this::escapeName).collect(Collectors.joining(","))
       + " LIMIT " + limit);
    parameters.forEach((name, value) -> builder.bind(name).to(value));
    return new ResultSetIterator(schema, transactionContext.executeQuery(builder.build()));
  }

  @Override
  public boolean compareAndSwap(Collection<Field<?>> keys, Field<?> oldValue,
                                Field<?> newValue) throws InvalidFieldException, IllegalArgumentException {
    if (oldValue.getFieldType() != newValue.getFieldType()) {
      throw new IllegalArgumentException(
        String.format("Field types of oldValue (%s) and newValue (%s) are not the same",
                      oldValue.getFieldType(), newValue.getFieldType()));
    }
    if (!oldValue.getName().equals(newValue.getName())) {
      throw new IllegalArgumentException(
        String.format("Trying to compare and swap different fields. Old Value = %s, New Value = %s",
                      oldValue, newValue));
    }
    if (schema.isPrimaryKeyColumn(oldValue.getName())) {
      throw new IllegalArgumentException("Cannot use compare and swap on a primary key field");
    }

    StructuredRow existing = read(keys, Collections.singleton(oldValue.getName())).orElse(null);

    // Check if the existing value is as expected in the oldValue
    if (!isFieldEquals(oldValue, existing)) {
      return false;
    }

    List<Field<?>> updateFields = new ArrayList<>(keys);
    updateFields.add(newValue);
    if (existing == null) {
      insert(updateFields);
    } else {
      update(updateFields);
    }
    return true;
  }

  @Override
  public void increment(Collection<Field<?>> keys,
                        String column, long amount) throws InvalidFieldException, IllegalArgumentException {
    if (schema.isPrimaryKeyColumn(column)) {
      throw new IllegalArgumentException("Cannot use increment on a primary key field");
    }
    FieldType.Type type = schema.getType(column);
    if (type == null) {
      throw new InvalidFieldException(schema.getTableId(), column, "Column " + column + " does not exist");
    }
    if (type != FieldType.Type.LONG) {
      throw new IllegalArgumentException(
        String.format("Trying to increment a column of type %s. Only %s column type can be incremented",
                      type, FieldType.Type.LONG));
    }
    fieldValidator.validatePrimaryKeys(keys, false);

    StructuredRow existing = read(keys, Collections.singleton(column)).orElse(null);
    List<Field<?>> fields = new ArrayList<>(keys);
    fields.add(Fields.longField(column,
                                amount + (existing == null ? 0L : Objects.requireNonNull(existing.getLong(column)))));

    if (existing == null) {
      // Insert a new row if there is no existing row
      insert(fields);
    } else {
      // Update the row by incrementing the amount
      update(fields);
    }
  }

  @Override
  public void delete(Collection<Field<?>> keys) throws InvalidFieldException {
    fieldValidator.validatePrimaryKeys(keys, false);
    String sql = "DELETE FROM " + escapeName(schema.getTableId().getName()) + " WHERE "
      + keys.stream().map(f -> escapeName(f.getName()) + " = @" + f.getName()).collect(Collectors.joining(" AND "));

    Statement statement = keys.stream()
      .reduce(Statement.newBuilder(sql),
              (builder, field) -> builder.bind(field.getName()).to(getValue(field)),
              (builder1, builder2) -> builder1)
      .build();

    transactionContext.executeUpdate(statement);
  }

  @Override
  public void deleteAll(Range range) throws InvalidFieldException {
    fieldValidator.validatePrimaryKeys(range.getBegin(), true);
    fieldValidator.validatePrimaryKeys(range.getEnd(), true);

    Map<String, Value> parameters = new HashMap<>();
    String condition = getRangeWhereClause(range, parameters);

    Statement.Builder builder = Statement.newBuilder("DELETE FROM " + escapeName(schema.getTableId().getName())
                                                       + " WHERE " + (condition.isEmpty() ? "true" : condition));
    parameters.forEach((name, value) -> builder.bind(name).to(value));
    transactionContext.executeUpdate(builder.build());
  }

  @Override
  public long count(Collection<Range> keyRanges) {
    try (ResultSet resultSet = transactionContext.executeQuery(getCountStatement(keyRanges))) {
      if (!resultSet.next()) {
        return 0L;
      }
      return resultSet.getCurrentRowAsStruct().getLong(0);
    }
  }

  @Override
  public void close() {
    // No-op
  }

  private Statement getCountStatement(Collection<Range> ranges) {
    Map<String, Value> parameters = new HashMap<>();
    String whereClause = getRangesWhereClause(ranges, parameters);

    if (whereClause == null) {
      return Statement.of("SELECT COUNT(*) FROM " + escapeName(schema.getTableId().getName()));
    }

    Statement.Builder builder = Statement.newBuilder("SELECT COUNT(*) FROM " + escapeName(schema.getTableId().getName())
                                                       + " WHERE " + whereClause);
    parameters.forEach((name, value) -> builder.bind(name).to(value));
    return builder.build();
  }

  private String getRangeWhereClause(Range range, Map<String, Value> parameters) {
    List<String> conditions = new ArrayList<>();
    int paramIndex = parameters.size();

    for (Field<?> field : range.getBegin()) {
      conditions.add(escapeName(field.getName())
                       + (range.getBeginBound() == Range.Bound.INCLUSIVE ? " >= " : " > ") + "@p_" + paramIndex);
      parameters.put("p_" + paramIndex, getValue(field));
      paramIndex++;
    }

    for (Field<?> field : range.getEnd()) {
      conditions.add(escapeName(field.getName())
                       + (range.getEndBound() == Range.Bound.INCLUSIVE ? " <= " : " < ") + "@p_" + paramIndex);
      parameters.put("p_" + paramIndex, getValue(field));
      paramIndex++;
    }

    return String.join(" AND ", conditions);
  }

  /**
   * Generates the WHERE clause for a set of ranges.
   *
   * @param ranges the set of ranges to query for
   * @param parameters the parameters name and value used in the WHERE clause
   * @return the WHERE clause or {@code null} if the ranges resulted in a full table query.
   */
  @Nullable
  private String getRangesWhereClause(Collection<Range> ranges, Map<String, Value> parameters) {
    // Validate all ranges. Also, find if there is any range that is open on both ends.
    // Also split the scans into actual range scans and singleton scan, which can be done via IN condition.
    Map<String, Set<Field<?>>> keyFields = new LinkedHashMap<>();
    List<Range> rangeScans = new ArrayList<>();
    boolean scanAll = false;
    for (Range range : ranges) {
      fieldValidator.validatePrimaryKeys(range.getBegin(), true);
      fieldValidator.validatePrimaryKeys(range.getEnd(), true);

      if (range.isSingleton()) {
        range.getBegin().forEach(f -> keyFields.computeIfAbsent(f.getName(), k -> new LinkedHashSet<>()).add(f));
      } else {
        if (range.getBegin().isEmpty() && range.getEnd().isEmpty()) {
          // We continue the loop so that all keyRanges are still validated
          scanAll = true;
        }
        rangeScans.add(range);
      }
    }
    if (scanAll) {
      return null;
    }

    // Generates "key1 in (?,?) AND key2 in (?,?)..." clause
    StringBuilder query = new StringBuilder();
    String separator = "";
    for (Map.Entry<String, Set<Field<?>>> entry : keyFields.entrySet()) {
      query.append(separator).append(entry.getKey()).append(" IN (");

      for (Field<?> field : entry.getValue()) {
        parameters.put("p_" + parameters.size(), getValue(field));
      }
      query.append(IntStream.range(0, parameters.size()).mapToObj(i -> "@p_" + i).collect(Collectors.joining(",")));
      query.append(")");
      separator = " AND ";
    }

    // Generates the ((key3 >= ?) AND (key3 <= ?)) OR ((key4 >= ?) AND (key4 <= ?))
    if (!rangeScans.isEmpty()) {
      separator = keyFields.isEmpty() ? "(" : " OR (";
      for (Range range : rangeScans) {
        query.append(separator).append("(").append(getRangeWhereClause(range, parameters)).append(")");
        separator = " OR ";
      }
      query.append(")");
    }

    return query.toString();
  }

  private void insert(Collection<Field<?>> fields) throws InvalidFieldException {
    List<Field<?>> insertFields = new ArrayList<>();
    for (Field<?> field : fields) {
      fieldValidator.validateField(field);
      insertFields.add(field);
    }

    String sql = "INSERT INTO " + escapeName(schema.getTableId().getName()) + " ("
      + insertFields.stream().map(Field::getName).map(this::escapeName).collect(Collectors.joining(",")) + ") VALUES ("
      + insertFields.stream().map(f -> "@" + f.getName()).collect(Collectors.joining(","))
      + ")";

    LOG.trace("Inserting row: {}", sql);

    Statement statement = fields.stream()
      .reduce(Statement.newBuilder(sql),
              (builder, field) -> builder.bind(field.getName()).to(getValue(field)),
              (builder1, builder2) -> builder1)
      .build();

    transactionContext.executeUpdate(statement);
  }

  private Key createKey(Collection<Field<?>> fields) {
    return Key.of(fields.stream().map(Field::getValue).toArray());
  }

  /**
   * Converts a {@link Field} into spanner {@link Value}.
   */
  @Nullable
  private Value getValue(Field<?> field) {
    Object value = field.getValue();
    if (value == null) {
      return null;
    }

    switch (field.getFieldType()) {
      case INTEGER:
        return Value.int64((int) value);
      case LONG:
        return Value.int64((long) value);
      case FLOAT:
        return Value.float64((float) value);
      case DOUBLE:
        return Value.float64((double) value);
      case STRING:
        return Value.string((String) value);
      case BYTES:
        return Value.bytes(ByteArray.copyFrom((byte[]) value));
    }

    // This shouldn't happen
    throw new IllegalArgumentException("Unsupported field type " + field.getFieldType());
  }

  /**
   * Converts a {@link Field} into spanner {@link Key}.
   */
  private Key getKey(Collection<Field<?>> fields) {
    Key.Builder builder = Key.newBuilder();

    for (Field<?> field : fields) {
      Object value = field.getValue();
      if (value == null) {
        return Key.of((Object) null);
      }

      switch (field.getFieldType()) {
        case INTEGER:
          builder.append((int) value).build();
          break;
        case LONG:
          builder.append((long) value).build();
          break;
        case FLOAT:
          builder.append((float) value).build();
          break;
        case DOUBLE:
          builder.append((double) value).build();
          break;
        case STRING:
          builder.append((String) value).build();
          break;
        case BYTES:
          builder.append(ByteArray.copyFrom((byte[]) value)).build();
          break;
        default:
          throw new IllegalArgumentException("Unsupported field type " + field.getFieldType());
      }
    }

    return builder.build();
  }

  private KeyRange.Endpoint getEndpoint(Range.Bound bound) {
    return bound == Range.Bound.INCLUSIVE ? KeyRange.Endpoint.CLOSED : KeyRange.Endpoint.OPEN;
  }

  private boolean isFieldEquals(Field<?> field, @Nullable StructuredRow row) {
    Object fieldValue = field.getValue();
    if (row == null) {
      return fieldValue == null;
    }

    switch (field.getFieldType()) {
      case INTEGER:
        return Objects.equals(fieldValue, row.getInteger(field.getName()));
      case LONG:
        return Objects.equals(fieldValue, row.getLong(field.getName()));
      case FLOAT:
        return Objects.equals(fieldValue, row.getFloat(field.getName()));
      case DOUBLE:
        return Objects.equals(fieldValue, row.getDouble(field.getName()));
      case STRING:
        return Objects.equals(fieldValue, row.getString(field.getName()));
      case BYTES:
        return Objects.deepEquals(fieldValue, row.getBytes(field.getName()));
    }
    return false;
  }

  /**
   * Returns a {@link String} in the form of {@code [tableName.]fieldName = @fieldName}.
   */
  private String fieldToParam(Field<?> field) {
    String fieldName = field.getName();
    if (fieldName.equalsIgnoreCase(schema.getTableId().getName())) {
      fieldName = escapeName(schema.getTableId().getName()) + "." + escapeName(fieldName);
    } else {
      fieldName = escapeName(fieldName);
    }
    return fieldName + " = @" + field.getName();
  }

  private String escapeName(String name) {
    return "`" + name + "`";
  }

  /**
   * A {@link CloseableIterator} to iterate over a Spanner {@link ResultSet}.
   */
  private static final class ResultSetIterator extends AbstractCloseableIterator<StructuredRow> {

    private final StructuredTableSchema schema;
    private final ResultSet resultSet;

    ResultSetIterator(StructuredTableSchema schema, ResultSet resultSet) {
      this.schema = schema;
      this.resultSet = resultSet;
    }

    @Override
    protected StructuredRow computeNext() {
      if (!resultSet.next()) {
        return endOfData();
      }
      return new SpannerStructuredRow(schema, resultSet.getCurrentRowAsStruct());
    }

    @Override
    public void close() {
      resultSet.close();
    }
  }
}
